{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Florian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"C:\\\\Users\\\\Florian\\\\Documents\\\\GitHub\\\\CovidSent\\\\Data\\\\train_data\"\n",
    "output_path = \"C:\\\\Users\\\\Florian\\\\Documents\\\\GitHub\\\\CovidSent\\\\Data\\\\output\"\n",
    "file = \"training.1600000.processed.noemoticon.csv\"\n",
    "\n",
    "final_file = \"output.csv\"\n",
    "output_file = \"output_sents.csv\"\n",
    "\n",
    "load = test_path + \"\\\\\" + file\n",
    "to_clf = output_path + \"\\\\\" + final_file\n",
    "out = output_path + \"\\\\\" + output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p < 1 only for testing purposes!\n",
    "# p = %of dataset used\n",
    "p = 1\n",
    "heads = ['sent', 'id', 'date', 'query', 'user', 'text']\n",
    "dataframe = pd.read_csv(load, encoding = \"ISO-8859-1\", engine='python', names=heads,\n",
    "                        skiprows=lambda i: i>0 and random.random() > p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = count.fit_transform(dataframe.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True,\n",
    "                         norm='l2',\n",
    "                         smooth_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf.fit_transform(bag_of_words).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataframe['text']\n",
    "y = dataframe['sent']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    \"\"\" Return a cleaned version of text\n",
    "    \"\"\"\n",
    "    # Remove HTML markup\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    # Save emoticons for later appending\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    # Remove any non-word character and append the emoticons,\n",
    "    # removing the nose character for standarization. Convert to lower case\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop],\n",
    "               'vect__tokenizer': [tokenizer_porter],\n",
    "               'vect__preprocessor': [preprocessor],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop],\n",
    "               'vect__tokenizer': [tokenizer_porter],\n",
    "               'vect__preprocessor': [preprocessor],\n",
    "               'vect__use_idf': [False],\n",
    "               'vect__norm': [None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0]},\n",
    "              ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=2,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfif = tfidf.transform(X_train)\n",
    "X_test_tfif = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.25, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=75, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full batch training\n",
    "X_train_tfif = tfidf.transform(X)\n",
    "logreg = LogisticRegression(random_state=0, verbose=1, C=1.25, penalty= \"l2\", max_iter=75)\n",
    "logreg.fit(X_train_tfif, y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv(to_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_x = tfidf.transform(target.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_y = logreg.predict(target_x)\n",
    "target_prob = logreg.predict_proba(target_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[\"prediction\"] = target_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_to_be_pos = []\n",
    "for i in target_prob:\n",
    "    prob_to_be_pos.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[\"pos_proba\"] = prob_to_be_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.4441\n",
       "1        0.3579\n",
       "2        0.7889\n",
       "3        0.8485\n",
       "4        0.5445\n",
       "5        0.6673\n",
       "6        0.3675\n",
       "7        0.7280\n",
       "8        0.7727\n",
       "9        0.9364\n",
       "10       0.7856\n",
       "11       0.6232\n",
       "12       0.4096\n",
       "13       0.9582\n",
       "14       0.7207\n",
       "15       0.9170\n",
       "16       0.4268\n",
       "17       0.8537\n",
       "18       0.6070\n",
       "19       0.6653\n",
       "20       0.7324\n",
       "21       0.8547\n",
       "22       0.5516\n",
       "23       0.7199\n",
       "24       0.6431\n",
       "25       0.8749\n",
       "26       0.2505\n",
       "27       0.4661\n",
       "28       0.4821\n",
       "29       0.5269\n",
       "          ...  \n",
       "18259    0.4621\n",
       "18260    0.7619\n",
       "18261    0.7851\n",
       "18262    0.6262\n",
       "18263    0.4170\n",
       "18264    0.6641\n",
       "18265    0.7554\n",
       "18266    0.8337\n",
       "18267    0.8337\n",
       "18268    0.6811\n",
       "18269    0.5448\n",
       "18270    0.2716\n",
       "18271    0.8871\n",
       "18272    0.8240\n",
       "18273    0.4178\n",
       "18274    0.5537\n",
       "18275    0.1377\n",
       "18276    0.6325\n",
       "18277    0.8875\n",
       "18278    0.5542\n",
       "18279    0.8172\n",
       "18280    0.6673\n",
       "18281    0.7701\n",
       "18282    0.7370\n",
       "18283    0.3310\n",
       "18284    0.8686\n",
       "18285    0.0408\n",
       "18286    0.8899\n",
       "18287    0.5874\n",
       "18288    0.5055\n",
       "Name: pos_proba, Length: 18289, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[\"pos_proba\"].round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.to_csv(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 4, ..., 4, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.prediction.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
